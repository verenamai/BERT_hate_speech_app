{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:12.666208Z",
     "start_time": "2020-03-06T14:55:11.698953Z"
    }
   },
   "outputs": [],
   "source": [
    "%run zzz_import_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:15.423262Z",
     "start_time": "2020-03-06T14:55:13.476968Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import unidecode\n",
    "import re\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import emoji\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "lemmatizer = Lemmatizer()\n",
    "import spacy\n",
    "import sklearn\n",
    "from num2words import num2words\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:15.483731Z",
     "start_time": "2020-03-06T14:55:15.480718Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"If max_cols is exceeded, switch to truncate view\"\n",
    "pd.set_option('display.max_columns', 5400)\n",
    "# \"The maximum width in characters of a column\"\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:16.391168Z",
     "start_time": "2020-03-06T14:55:16.317512Z"
    }
   },
   "outputs": [],
   "source": [
    "# before extracting hashtags from tweets, we need to ride of html parts which are \n",
    "# in tweets like: &#128111;&#128131;&#128131;\n",
    "import html\n",
    "df_tweets_all['tweet_no_html'] = df_tweets_all['tweet'].apply(html.unescape)\n",
    "#df_tweets_all['tweet_lower'] = df_tweets_all['tweet_no_html'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:16.716116Z",
     "start_time": "2020-03-06T14:55:16.712605Z"
    }
   },
   "outputs": [],
   "source": [
    "# before start text cleaning lets look at used hashtags\n",
    "# function to collect hashtags\n",
    "def hashtag_extract(x):\n",
    "    x = x.str.lower()\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:17.180225Z",
     "start_time": "2020-03-06T14:55:17.176569Z"
    }
   },
   "outputs": [],
   "source": [
    "def emojis_extract(x):\n",
    "    emojies = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        em = re.findall(r\":(\\w+):\", i)\n",
    "        emojies.append(em)\n",
    "    return emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:17.678978Z",
     "start_time": "2020-03-06T14:55:17.675688Z"
    }
   },
   "outputs": [],
   "source": [
    "def wrap_emojies(emoji_list):\n",
    "    emojies = []\n",
    "    for emo in emoji_list:\n",
    "        emojies.append(emoji.emojize(emo))\n",
    "    return emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:18.152148Z",
     "start_time": "2020-03-06T14:55:18.132768Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern_retweet = r'^rt '\n",
    "pattern_char = r'@[A-Za-z0-9_]+'\n",
    "pattern_html = r'https?://[^ ]+'\n",
    "pattern_combi = r'|'.join((pattern_char, pattern_html))\n",
    "pattern_www = r'www.[^ ]+'\n",
    "negations_dic = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"luv\" : \"love\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"yo\" : \"you\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"yu\" : \"you\" }\n",
    "pattern_neg = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:19.474127Z",
     "start_time": "2020-03-06T14:55:19.469485Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern_replace_url = re.compile(r'(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)')\n",
    "old = 'hello world this https://www.gooogle.ch is a sentence www.google.com with google.ch urls http://www.bleuwin.ch'\n",
    "\n",
    "new_ip = 'replaced_url'\n",
    "\n",
    "replaced_url = re.sub(pattern_replace_url, new_ip, old)\n",
    "\n",
    "print('replaced = %s' %(replaced_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:19.791381Z",
     "start_time": "2020-03-06T14:55:19.788502Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_url(input_text):\n",
    "    replaced_url = re.sub(pattern_replace_url, new_ip, input_text)\n",
    "    return replaced_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:20.138205Z",
     "start_time": "2020-03-06T14:55:20.132139Z"
    }
   },
   "outputs": [],
   "source": [
    "replace_url(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:22.282285Z",
     "start_time": "2020-03-06T14:55:22.277570Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(input_text):\n",
    "    #print(\"raw text: \\n{} \\n\".format(input_text))\n",
    "    decoded = ftfy.fix_encoding(input_text)\n",
    "    #print(\"decoded: \\n{} \\n\".format(decoded))\n",
    "    #lower = decoded.lower()\n",
    "    #print(\"lower: \\n{} \\n\".format(lower))\n",
    "    #html_free = re.sub(pattern_combi, '', lower)\n",
    "    #url_free = re.sub(pattern_www, '', html_free)\n",
    "    #print(\"html and url free: \\n{}\\n\".format(url_free))\n",
    "    neg_handled = pattern_neg.sub(lambda x: negations_dic[x.group()], decoded)\n",
    "    #print(\"neg_handled: \\n{}\\n\".format(neg_handled))\n",
    "    demo = emoji.demojize(neg_handled)\n",
    "    #to do sonderzeichen remove\n",
    "    # remove special characters, numbers, punctuations aber erst nach rettung der emojies -> new function\n",
    "    #combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    ## remove twitter handles (@user)\n",
    "    #combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")\n",
    "    return demo\n",
    "    #stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    #stripped = re.sub(www_pat, '', stripped)\n",
    "    #lower_case = stripped.lower()\n",
    "    #neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:23.194948Z",
     "start_time": "2020-03-06T14:55:23.190774Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_signs(input_text):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", input_text)\n",
    "    n_rt = re.sub(pattern_retweet, ' ', letters_only)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(n_rt) if len(x) > 1]\n",
    "    #stemmed = [x for x  in stemmer.stem(words)]\n",
    "    #stemmed = [stemmer.stem(word) for word in words]\n",
    "    #print(stemmed)\n",
    "    #print(words)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:23.541847Z",
     "start_time": "2020-03-06T14:55:23.538383Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemm(input_text):\n",
    "    doc = nlp(input_text)\n",
    "    # Extract the lemma for each token and join\n",
    "    return(\" \".join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:24.035584Z",
     "start_time": "2020-03-06T14:55:24.026877Z"
    }
   },
   "outputs": [],
   "source": [
    "def spacy_cleaner(input_text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(input_text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(input_text)\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    expanded = ' '.join([negations_dic[t] if t in negations_dic else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:24.546663Z",
     "start_time": "2020-03-06T14:55:24.542538Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text, stopword_list):\n",
    "    '''a function for removing the stopword'''\n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    text = [word.lower() for word in input_text.split() if word.lower() not in stopword_list]\n",
    "    # joining the list of words with space separator\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:56:49.002624Z",
     "start_time": "2020-03-06T14:56:48.973255Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tweets_all.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:55:34.894587Z",
     "start_time": "2020-03-06T14:55:33.753144Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tweets_all['no_url'] = df_tweets_all['tweet_no_html'].apply(replace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:56:04.962189Z",
     "start_time": "2020-03-06T14:55:34.962091Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tweets_all['tweet_tidy_1'] = df_tweets_all['no_url'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:56:59.185779Z",
     "start_time": "2020-03-06T14:56:59.085915Z"
    }
   },
   "outputs": [],
   "source": [
    "int(df_tweets_all['tweet_tidy_1'].str.encode(encoding='utf-8').str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:58:31.641400Z",
     "start_time": "2020-03-06T14:58:31.555288Z"
    }
   },
   "outputs": [],
   "source": [
    "int(df_tweets_all['tweet_tidy_1'].str.encode(encoding='utf-8').str.len().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:56:05.028106Z",
     "start_time": "2020-03-06T14:56:05.024611Z"
    }
   },
   "outputs": [],
   "source": [
    "print('done with preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
